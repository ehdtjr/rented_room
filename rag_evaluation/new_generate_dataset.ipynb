{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "# logging.langsmith(\"rag_evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "import csv\n",
    "\n",
    "file_path = \"../data/csv_data/rental_data_with_null.csv\"\n",
    "\n",
    "\n",
    "def get_csv_headers(file_path):\n",
    "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        headers = next(reader)  # 첫 번째 줄(헤더) 가져오기\n",
    "    return headers\n",
    "\n",
    "\n",
    "headers = get_csv_headers(file_path)\n",
    "\n",
    "# CSV 로더 생성\n",
    "loader = CSVLoader(\n",
    "    file_path=file_path,\n",
    "    csv_args={\n",
    "        \"delimiter\": \",\",\n",
    "        \"quotechar\": '\"',\n",
    "        \"fieldnames\": headers,\n",
    "    },\n",
    "    # source_column=\"place\",\n",
    "    content_columns=headers,\n",
    "    metadata_columns=[\n",
    "        \"place\",\n",
    "        \"oneroom_half_year\",\n",
    "        \"oneroom_year\",\n",
    "        \"tworoom_half_year\",\n",
    "        \"tworoom_year\",\n",
    "    ],\n",
    ")\n",
    "docs = loader.load()\n",
    "print(docs[1].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "\n",
    "for doc in docs[i:]:\n",
    "    row = doc.page_content.split(\"\\n\")\n",
    "    row_str = \"<row>\"\n",
    "    for element in row:\n",
    "        splitted_element = element.split(\":\")\n",
    "        value = splitted_element[-1]\n",
    "        col = \":\".join(splitted_element[:-1])\n",
    "        row_str += f\"<{col}>{value.strip()}</{col}>\"\n",
    "    row_str += \"</row>\\n\\n\"\n",
    "\n",
    "    docs[i].page_content = row_str\n",
    "    i += 1\n",
    "    # print(ret[i].page_content)\n",
    "    # ret += row_str\n",
    "\n",
    "ret = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 각 문서에 파일 이름을 추가합니다.\n",
    "# for i in ret[1:]:\n",
    "#     i.metadata[\"filename\"] = i.metadata[\"source\"]\n",
    "\n",
    "ret[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.persona import Persona\n",
    "\n",
    "persona_manager = Persona(\n",
    "    name=\"Manager\",\n",
    "    role_description=\"Wants to know about the information related to my own room.\",\n",
    ")\n",
    "\n",
    "personas = [persona_manager]\n",
    "personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(\n",
    "    OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(\n",
    "    llm=generator_llm, embedding_model=generator_embeddings, persona_list=personas\n",
    ")\n",
    "\n",
    "# dataset = generator.generate_with_langchain_docs(ret[1:], testset_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.synthesizers.single_hop.specific import (\n",
    "    SingleHopSpecificQuerySynthesizer,\n",
    ")\n",
    "\n",
    "distribution = [\n",
    "    (SingleHopSpecificQuerySynthesizer(llm=generator_llm), 1.0),\n",
    "]\n",
    "\n",
    "for query, _ in distribution:\n",
    "    prompts = await query.adapt_prompts(\"korean\", llm=generator_llm)\n",
    "    query.set_prompts(**prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = generator.generate_with_langchain_docs(\n",
    "    ret[1:], testset_size=10, query_distribution=distribution\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pandas().to_csv(\"../data/csv_data/new_ragas_dataset_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 절대 경로 설정\n",
    "csv_file_path = \"../data/csv_data/new_ragas_dataset.csv\"\n",
    "\n",
    "# 로컬 CSV 파일로 데이터셋 로드\n",
    "dataset = load_dataset(\"csv\", data_files=csv_file_path)\n",
    "\n",
    "# 데이터셋 정보 출력\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset(\n",
    "#     \"explodinggradients/amnesty_qa\",\n",
    "#     \"english_v3\",\n",
    "# )\n",
    "# print(dataset)\n",
    "# print(dataset[\"eval\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "\n",
    "eval_dataset = EvaluationDataset.from_hf_dataset(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    LLMContextRecall,\n",
    "    Faithfulness,\n",
    "    FactualCorrectness,\n",
    "    SemanticSimilarity,\n",
    ")\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from ragas import evaluate\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(\n",
    "    OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    LLMContextRecall(llm=evaluator_llm),\n",
    "    FactualCorrectness(llm=evaluator_llm),\n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    SemanticSimilarity(embeddings=evaluator_embeddings),\n",
    "]\n",
    "\n",
    "results = evaluate(dataset=eval_dataset, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
