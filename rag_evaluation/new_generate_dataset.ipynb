{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "# logging.langsmith(\"rag_evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "import csv\n",
    "\n",
    "file_path = \"../data/csv_data/rental_data_with_null.csv\"\n",
    "\n",
    "\n",
    "def get_csv_headers(file_path):\n",
    "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        headers = next(reader)  # 첫 번째 줄(헤더) 가져오기\n",
    "    return headers\n",
    "\n",
    "\n",
    "headers = get_csv_headers(file_path)\n",
    "\n",
    "# CSV 로더 생성\n",
    "loader = CSVLoader(\n",
    "    file_path=file_path,\n",
    "    csv_args={\n",
    "        \"delimiter\": \",\",\n",
    "        \"quotechar\": '\"',\n",
    "        \"fieldnames\": headers,\n",
    "    },\n",
    "    # source_column=\"place\",\n",
    "    content_columns=headers,\n",
    "    metadata_columns=[\n",
    "        \"place\",\n",
    "        \"oneroom_half_year\",\n",
    "        \"oneroom_year\",\n",
    "        \"tworoom_half_year\",\n",
    "        \"tworoom_year\",\n",
    "    ],\n",
    ")\n",
    "docs = loader.load()\n",
    "print(docs[1].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "\n",
    "for doc in docs[i:]:\n",
    "    row = doc.page_content.split(\"\\n\")\n",
    "    row_str = \"<row>\"\n",
    "    for element in row:\n",
    "        splitted_element = element.split(\":\")\n",
    "        value = splitted_element[-1]\n",
    "        col = \":\".join(splitted_element[:-1])\n",
    "        row_str += f\"<{col}>{value.strip()}</{col}>\"\n",
    "    row_str += \"</row>\\n\\n\"\n",
    "\n",
    "    docs[i].page_content = row_str\n",
    "    i += 1\n",
    "    # print(ret[i].page_content)\n",
    "    # ret += row_str\n",
    "\n",
    "ret = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 각 문서에 파일 이름을 추가합니다.\n",
    "# for i in ret[1:]:\n",
    "#     i.metadata[\"filename\"] = i.metadata[\"source\"]\n",
    "\n",
    "ret[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.persona import Persona\n",
    "\n",
    "persona_manager = Persona(\n",
    "    name=\"Manager\",\n",
    "    role_description=\"Wants to know about the information related to my own room.\",\n",
    ")\n",
    "\n",
    "personas = [persona_manager]\n",
    "personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(\n",
    "    OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(\n",
    "    llm=generator_llm, embedding_model=generator_embeddings, persona_list=personas\n",
    ")\n",
    "\n",
    "# dataset = generator.generate_with_langchain_docs(ret[1:], testset_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.synthesizers.single_hop.specific import (\n",
    "    SingleHopSpecificQuerySynthesizer,\n",
    ")\n",
    "\n",
    "distribution = [\n",
    "    (SingleHopSpecificQuerySynthesizer(llm=generator_llm), 1.0),\n",
    "]\n",
    "\n",
    "for query, _ in distribution:\n",
    "    prompts = await query.adapt_prompts(\"korean\", llm=generator_llm)\n",
    "    query.set_prompts(**prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = generator.generate_with_langchain_docs(\n",
    "    ret[1:], testset_size=10, query_distribution=distribution\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pandas().to_csv(\"../data/csv_data/new_ragas_dataset_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['user_input', 'reference_contexts', 'reference', 'synthesizer_name'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 절대 경로 설정\n",
    "csv_file_path = \"../data/csv_data/new_ragas_dataset.csv\"\n",
    "\n",
    "# 로컬 CSV 파일로 데이터셋 로드\n",
    "dataset = load_dataset(\"csv\", data_files=csv_file_path)\n",
    "\n",
    "# 데이터셋 정보 출력\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset(\n",
    "#     \"explodinggradients/amnesty_qa\",\n",
    "#     \"english_v3\",\n",
    "# )\n",
    "# print(dataset)\n",
    "# print(dataset[\"eval\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for SingleTurnSample\nreference_contexts\n  Input should be a valid list [type=list_type, input_value=\"['<row><name>우리오...oom_year></row>\\\\n\\\\n']\", input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/list_type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvaluationDataset\n\u001b[0;32m----> 3\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mEvaluationDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_hf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Workspace/rented_room/.venv/lib/python3.11/site-packages/ragas/dataset_schema.py:205\u001b[0m, in \u001b[0;36mRagasDataset.from_hf_dataset\u001b[0;34m(cls, dataset)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_hf_dataset\u001b[39m(\u001b[38;5;28mcls\u001b[39m: t\u001b[38;5;241m.\u001b[39mType[T], dataset: HFDataset) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates an EvaluationDataset from a Hugging Face Dataset.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Workspace/rented_room/.venv/lib/python3.11/site-packages/ragas/dataset_schema.py:361\u001b[0m, in \u001b[0;36mEvaluationDataset.from_list\u001b[0;34m(cls, data)\u001b[0m\n\u001b[1;32m    359\u001b[0m     samples\u001b[38;5;241m.\u001b[39mextend(MultiTurnSample(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msample) \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data)\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 361\u001b[0m     \u001b[43msamples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSingleTurnSample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(samples\u001b[38;5;241m=\u001b[39msamples)\n",
      "File \u001b[0;32m~/Desktop/Workspace/rented_room/.venv/lib/python3.11/site-packages/ragas/dataset_schema.py:361\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    359\u001b[0m     samples\u001b[38;5;241m.\u001b[39mextend(MultiTurnSample(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msample) \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data)\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 361\u001b[0m     samples\u001b[38;5;241m.\u001b[39mextend(\u001b[43mSingleTurnSample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data)\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(samples\u001b[38;5;241m=\u001b[39msamples)\n",
      "File \u001b[0;32m~/Desktop/Workspace/rented_room/.venv/lib/python3.11/site-packages/pydantic/main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    220\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    221\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for SingleTurnSample\nreference_contexts\n  Input should be a valid list [type=list_type, input_value=\"['<row><name>우리오...oom_year></row>\\\\n\\\\n']\", input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/list_type"
     ]
    }
   ],
   "source": [
    "from ragas import EvaluationDataset\n",
    "\n",
    "eval_dataset = EvaluationDataset.from_hf_dataset(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The metric [factual_correctness] that is used requires the following additional columns ['response'] to be present in the dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 24\u001b[0m\n\u001b[1;32m     14\u001b[0m evaluator_embeddings \u001b[38;5;241m=\u001b[39m LangchainEmbeddingsWrapper(\n\u001b[1;32m     15\u001b[0m     OpenAIEmbeddings(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-embedding-3-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     19\u001b[0m     FactualCorrectness(llm\u001b[38;5;241m=\u001b[39mevaluator_llm),\n\u001b[1;32m     20\u001b[0m     Faithfulness(llm\u001b[38;5;241m=\u001b[39mevaluator_llm),\n\u001b[1;32m     21\u001b[0m     SemanticSimilarity(embeddings\u001b[38;5;241m=\u001b[39mevaluator_embeddings),\n\u001b[1;32m     22\u001b[0m ]\n\u001b[0;32m---> 24\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Workspace/rented_room/.venv/lib/python3.11/site-packages/ragas/_analytics.py:227\u001b[0m, in \u001b[0;36mtrack_was_completed.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m    226\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m--> 227\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Desktop/Workspace/rented_room/.venv/lib/python3.11/site-packages/ragas/evaluation.py:177\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar)\u001b[0m\n\u001b[1;32m    174\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m EvaluationDataset\u001b[38;5;241m.\u001b[39mfrom_list(dataset\u001b[38;5;241m.\u001b[39mto_list())\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, EvaluationDataset):\n\u001b[0;32m--> 177\u001b[0m     \u001b[43mvalidate_required_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     validate_supported_metrics(dataset, metrics)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# set the llm and embeddings\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Workspace/rented_room/.venv/lib/python3.11/site-packages/ragas/validation.py:63\u001b[0m, in \u001b[0;36mvalidate_required_columns\u001b[0;34m(ds, metrics)\u001b[0m\n\u001b[1;32m     61\u001b[0m available_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(ds\u001b[38;5;241m.\u001b[39mfeatures())\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m required_columns\u001b[38;5;241m.\u001b[39missubset(available_columns):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe metric [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] that is used requires the following \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional columns \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(required_columns\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mavailable_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto be present in the dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The metric [factual_correctness] that is used requires the following additional columns ['response'] to be present in the dataset."
     ]
    }
   ],
   "source": [
    "from ragas.metrics import (\n",
    "    LLMContextRecall,\n",
    "    Faithfulness,\n",
    "    FactualCorrectness,\n",
    "    SemanticSimilarity,\n",
    ")\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from ragas import evaluate\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(\n",
    "    OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    LLMContextRecall(llm=evaluator_llm),\n",
    "    FactualCorrectness(llm=evaluator_llm),\n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    SemanticSimilarity(embeddings=evaluator_embeddings),\n",
    "]\n",
    "\n",
    "results = evaluate(dataset=eval_dataset, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
